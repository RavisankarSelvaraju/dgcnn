{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2022-08-12 11:00:07,895 - utils - Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO - 2022-08-12 11:00:07,895 - utils - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--exp_name N] [--model N] [--dataset N]\n",
      "                             [--batch_size batch_size]\n",
      "                             [--test_batch_size batch_size] [--epochs N]\n",
      "                             [--use_sgd USE_SGD] [--lr LR] [--momentum M]\n",
      "                             [--no_cuda NO_CUDA] [--seed S] [--eval EVAL]\n",
      "                             [--num_points NUM_POINTS] [--dropout DROPOUT]\n",
      "                             [--emb_dims N] [--k N] [--model_path N]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jovyan/.local/share/jupyter/runtime/kernel-567a26e6-7b36-4929-adca-92907bae8e0f.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@Author: Yue Wang\n",
    "@Contact: yuewangx@mit.edu\n",
    "@File: main.py\n",
    "@Time: 2018/10/13 10:39 PM\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "from tarfile import TarInfo\n",
    "from sklearn import cluster\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from data import nagoya_dataset\n",
    "from model import PointNet, DGCNN\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from util import cal_loss, IOStream\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "\n",
    "def _init_():\n",
    "\n",
    "    if not os.path.exists('checkpoints'):\n",
    "        os.makedirs('checkpoints')\n",
    "    if not os.path.exists('checkpoints/'+args.exp_name):\n",
    "        os.makedirs('checkpoints/'+args.exp_name)\n",
    "    if not os.path.exists('checkpoints/'+args.exp_name+'/'+'models'):\n",
    "        os.makedirs('checkpoints/'+args.exp_name+'/'+'models')\n",
    "\n",
    "    # os.system('cp main.py checkpoints'+'/'+args.exp_name+'/'+'main.py.backup')\n",
    "    # os.system('cp model.py checkpoints' + '/' + args.exp_name + '/' + 'model.py.backup')\n",
    "    # os.system('cp util.py checkpoints' + '/' + args.exp_name + '/' + 'util.py.backup')\n",
    "    # os.system('cp data.py checkpoints' + '/' + args.exp_name + '/' + 'data.py.backup')\n",
    "\n",
    "    print(\"====init====\")     \n",
    "\n",
    "def train(args, io):\n",
    "\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(nagoya_dataset(partition='train', num_points=args.num_points), num_workers=2,\n",
    "                              batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(nagoya_dataset(partition='test', num_points=args.num_points), num_workers=2,\n",
    "                             batch_size=args.test_batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    #Try to load models\n",
    "    if args.model == 'pointnet':\n",
    "        model = PointNet(args).to(device)\n",
    "    elif args.model == 'dgcnn':\n",
    "        model = DGCNN(args).to(device)\n",
    "    else:\n",
    "        raise Exception(\"Not implemented\")\n",
    "    print(str(model))\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "\n",
    "    if args.use_sgd:\n",
    "        print(\"Use SGD\")\n",
    "        opt = optim.SGD(model.parameters(), lr=args.lr*100, momentum=args.momentum, weight_decay=1e-4)\n",
    "    else:\n",
    "        print(\"Use Adam\")\n",
    "        opt = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "\n",
    "    scheduler = CosineAnnealingLR(opt, args.epochs, eta_min=args.lr)\n",
    "    \n",
    "    criterion = cal_loss\n",
    "\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    training_acc = []\n",
    "    valid_acc = []\n",
    "    training_loss = []\n",
    "    valid_loss = []\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        \n",
    "        ####################\n",
    "        # Train\n",
    "        ####################\n",
    "        train_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.train()\n",
    "        train_pred = []\n",
    "        train_true = []\n",
    "        print(\"\\n\")\n",
    "        print(f'++++++++ ++++++++  ++++++++  ++++++++  START of Epoch : {epoch} ++++++++  ++++++++  ++++++++  ++++++++')\n",
    "        print(\"Training ...\")\n",
    "        for data, label in train_loader:\n",
    "\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            data = data.permute(0, 2, 1)\n",
    "\n",
    "            batch_size = data.size()[0]\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # print(\"*\", end=\" \")\n",
    "\n",
    "            # print(\"printing data shape\")\n",
    "            # print(data.shape)\n",
    "\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            preds = logits.max(dim=1)[1]\n",
    "            count += batch_size\n",
    "            train_loss += loss.item() * batch_size\n",
    "            train_true.append(label.cpu().numpy())\n",
    "            train_pred.append(preds.detach().cpu().numpy())\n",
    "\n",
    "        train_true = np.concatenate(train_true)\n",
    "        train_pred = np.concatenate(train_pred)\n",
    "        scheduler.step()\n",
    "        outstr = 'Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f' % (epoch,\n",
    "                                                                                 train_loss*1.0/count,\n",
    "                                                                                 metrics.accuracy_score(\n",
    "                                                                                     train_true, train_pred),\n",
    "                                                                                 metrics.balanced_accuracy_score(\n",
    "                                                                                     train_true, train_pred))\n",
    "\n",
    "        training_acc.append(metrics.accuracy_score(train_true, train_pred))\n",
    "        training_loss.append(train_loss*1.0/count)\n",
    "\n",
    "        io.cprint(outstr)\n",
    "        print(\"\\n\")\n",
    "        ####################\n",
    "        # Test\n",
    "        ####################\n",
    "        test_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_true = []\n",
    "        print(\"Validating ...\")\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            data = data.permute(0, 2, 1)\n",
    "            batch_size = data.size()[0]\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, label)\n",
    "            preds = logits.max(dim=1)[1]\n",
    "            count += batch_size\n",
    "            test_loss += loss.item() * batch_size\n",
    "            test_true.append(label.cpu().numpy())\n",
    "            test_pred.append(preds.detach().cpu().numpy())\n",
    "            # print(\"*\", end=\" \")\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "        test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "        avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "        # print(\"\\n\")\n",
    "        outstr = 'Valdation %d, loss: %.6f, validation acc: %.6f, validation avg acc: %.6f' % (epoch,\n",
    "                                                                              test_loss*1.0/count,\n",
    "                                                                              test_acc,\n",
    "                                                                              avg_per_class_acc)\n",
    "\n",
    "        valid_acc.append(test_acc)\n",
    "        valid_loss.append(test_loss*1.0/count)\n",
    "\n",
    "        io.cprint(outstr)\n",
    "        print(\"\\n\")\n",
    "        print(f'========== ========== ========== ========== End of Epoch : {epoch} ========== ========== ========== ==========')\n",
    "\n",
    "        if test_acc >= best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'checkpoints/%s/models/model.t7' % args.exp_name)\n",
    "\n",
    "    pd.DataFrame({\"train_acc\": training_acc, \"valid_acc\": valid_acc, \"train_loss\": training_loss, \"valid_loss\": valid_loss}).to_csv(\"checkpoints/%s/models/resuts.csv\" % args.exp_name)\n",
    "\n",
    "\n",
    "def test(args, io):\n",
    "    test_loader = DataLoader(nagoya_dataset(partition='test', num_points=args.num_points),\n",
    "                             batch_size=args.test_batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    #Try to load models\n",
    "    model = DGCNN(args).to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(args.model_path))\n",
    "    model = model.eval()\n",
    "    test_acc = 0.0\n",
    "    count = 0.0\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "\n",
    "    print(\"Testing ...\")\n",
    "\n",
    "    for data, label in test_loader:\n",
    "\n",
    "        data, label = data.to(device), label.to(device).squeeze()\n",
    "        data = data.permute(0, 2, 1)\n",
    "        batch_size = data.size()[0]\n",
    "        logits = model(data)\n",
    "        preds = logits.max(dim=1)[1]\n",
    "        test_true.append(label.cpu().numpy())\n",
    "        test_pred.append(preds.detach().cpu().numpy())\n",
    "    test_true = np.concatenate(test_true)\n",
    "    test_pred = np.concatenate(test_pred)\n",
    "    test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "    avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "    outstr = 'Test :: test acc: %.6f, test avg acc: %.6f'%(test_acc, avg_per_class_acc)\n",
    "    io.cprint(outstr)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='Point Cloud Recognition')\n",
    "    parser.add_argument('--exp_name', type=str, default='exp', metavar='N',\n",
    "                        help='Name of the experiment')\n",
    "    parser.add_argument('--model', type=str, default='dgcnn', metavar='N',\n",
    "                        choices=['pointnet', 'dgcnn'],\n",
    "                        help='Model to use, [pointnet, dgcnn]')\n",
    "    parser.add_argument('--dataset', type=str, default='nagoya_dataset', metavar='N',\n",
    "                        choices=['nagoya_dataset'])\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=4, metavar='batch_size',\n",
    "                        help='Size of batch)')\n",
    "    parser.add_argument('--test_batch_size', type=int, default=4, metavar='batch_size',\n",
    "                        help='Size of batch)')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                        help='number of episode to train ')\n",
    "    parser.add_argument('--use_sgd', type=bool, default=True,\n",
    "                        help='Use SGD')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.001, 0.1 if using sgd)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                        help='SGD momentum (default: 0.9)')\n",
    "    parser.add_argument('--no_cuda', type=bool, default=False,\n",
    "                        help='enables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--eval', type=bool,  default=False,\n",
    "                        help='evaluate the model')\n",
    "    parser.add_argument('--num_points', type=int, default=1024,\n",
    "                        help='num of points to use')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                        help='dropout rate')\n",
    "    parser.add_argument('--emb_dims', type=int, default=1024, metavar='N',\n",
    "                        help='Dimension of embeddings')\n",
    "    parser.add_argument('--k', type=int, default=20, metavar='N',\n",
    "                        help='Num of nearest neighbors to use')\n",
    "    parser.add_argument('--model_path', type=str, default='', metavar='N',\n",
    "                        help='Pretrained model path')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    _init_()\n",
    "\n",
    "    # print(args)\n",
    "\n",
    "    # cluster_run = True\n",
    "\n",
    "    io = IOStream('checkpoints/' + args.exp_name + '/run.log')\n",
    "\n",
    "    # if cluster_run:\n",
    "    #     io = IOStream('/scratch/rselva2s/bit-bots/dgcnn/checkpoints/exp/run.log')\n",
    "    # else:\n",
    "    #     io = IOStream('/media/ravi/ubuntu_disk/ravi/atwork/other_repo/dgcnn/pytorch/checkpoints/dgcnn_1024/run.log')\n",
    "\n",
    "    io.cprint(str(args))\n",
    "\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        io.cprint(\n",
    "            'Using GPU : ' + str(torch.cuda.current_device()) + ' from ' + str(torch.cuda.device_count()) + ' devices')\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    else:\n",
    "        io.cprint('Using CPU')\n",
    "\n",
    "    if not args.eval:\n",
    "        train(args, io)\n",
    "        \n",
    "    else:\n",
    "        test(args, io)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 223>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--k\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, metavar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    260\u001b[0m                     help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNum of nearest neighbors to use\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    261\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--model_path\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, metavar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    262\u001b[0m                     help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPretrained model path\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 263\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m _init_()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# print(args)\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# cluster_run = True\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/argparse.py:1828\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argv:\n\u001b[1;32m   1827\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized arguments: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1828\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/argparse.py:2582\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_usage(_sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m   2581\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[0;32m-> 2582\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/argparse.py:2569\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message:\n\u001b[1;32m   2568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m-> 2569\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d31e505ffb8a0a69578125024a57b8ddf0c676a58e019d1a5a118f317ea97889"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
